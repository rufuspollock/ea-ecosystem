---
title: "Forethought: A new AI macrostrategy group ‚Äî EA Forum"
source: "https://forum.effectivealtruism.org/posts/6JnTAifyqz245Kv7S/forethought-a-new-ai-macrostrategy-group"
author:
  - "[[Amrit Sidhu-Brar üî∏]]"
  - "[[MaxDalton]]"
  - "[[William_MacAskill]]"
  - "[[Tom_Davidson]]"
  - "[[Forethought]]"
published: 2025-03-11
created: 2025-10-21
description: "Forethought[1] is a new AI macrostrategy research group cofounded by Max Dalton, Will MacAskill, Tom Davidson, and Amrit Sidhu-Brar. ‚Ä¶"
tags:
  - "clippings"
---
[**Forethought**](https://www.forethought.org/) **[^1] is a new AI macrostrategy research group** cofounded by Max Dalton, Will MacAskill, Tom Davidson, and Amrit Sidhu-Brar.

We are trying to figure out how to navigate the (potentially rapid) transition to a world with superintelligent AI systems. We aim to tackle the most important questions we can find, unrestricted by the current Overton window.

More details on our [website](https://www.forethought.org/).

## Why we exist

We think that AGI might come soon (say, modal timelines to mostly-automated AI R&D in the next 2-8 years), and might significantly accelerate technological progress, leading to many different challenges. We don‚Äôt yet have a good understanding of what this change might look like or how to navigate it. Society is not prepared.

Moreover, we want the world to not just avoid catastrophe: we want to reach a really great future. We think about what this might be like (incorporating moral uncertainty), and what we can do, now, to build towards a good future.

Like all projects, this started out with a plethora of Google docs. We ran a series of seminars to explore the ideas further, and that cascaded into an organization.

This area of work feels to us like the early days of EA: we‚Äôre exploring unusual, neglected ideas, and finding research progress surprisingly tractable. And while we start out with (literally) galaxy-brained schemes, they often ground out into fairly specific and concrete ideas about what should happen next. Of course, we‚Äôre bringing principles like scope sensitivity, impartiality, etc to our thinking, and we think that these issues urgently need more morally dedicated and thoughtful people working on them.

## Research

## Research agendas

We are currently pursuing the following perspectives:

- **Preparing for the intelligence explosion:** If AI drives explosive growth there will be an enormous number of challenges we have to face. In addition to misalignment risk and biorisk, this potentially includes: how to govern the development of new weapons of mass destruction; what rights to give digital beings; how to govern an automated military; and how to avoid dictatorship or authoritarianism.
- **Achieving a near-best future:** Most explicitly longtermist work to date has been focused on avoiding existential catastrophe, but achieving a near-best future might be even more important. Research avenues here include mapping out what a desirable ‚Äúviatopia‚Äù would look like (i.e. a state of the world which is very likely to lead to a very good future), figuring out how space resources should be allocated, and addressing issues around AI epistemics and persuasion.

We tend to think that many non-alignment areas of work are particularly neglected.

However, we are not confident that these are the best frames for this work, and we are keen to work with people who are pursuing their own agendas.

## Recent work

Today we‚Äôre also launching ‚Äú [Preparing for the Intelligence Explosion](https://www.forethought.org/research/preparing-for-the-intelligence-explosion) ‚Äù, which makes a more in-depth case for some of the perspectives above.

You can see some of our other recent work on the site. We have a backlog of research, so we‚Äôll be publishing something new every few days for the next few weeks.

## Approach

## Comparison to other efforts

We draw inspiration from the Future of Humanity Institute and from OpenPhil‚Äôs Worldview Investigations team: like them we aim to focus on big picture important questions, have high intellectual standards, and build a strong core team.

Generally, we‚Äôre more focused than many existing organizations on:

- Explosive growth and short timelines
- Work outside the current Overton window
- Issues beyond AI alignment, including new technologies and challenges that AI will unleash
- Reaching really good futures

## Principles

1. **Stay small:** We aim to hire from among the handful of people who have the best records of tackling hard questions about AI futures, to offer a supportive institutional home to such people, and to grow slowly.
2. **Communicate to the nerds:** We will mostly share research and ideas with wonk-y folks thinking about AI in think tanks, companies, and government, rather than working directly with policymakers. We plan to be thoughtful about how best to communicate and publish, but likely on our website and in arXiv papers.
3. **Be open to ‚Äúweird‚Äù ideas:** The most important ideas in history often seemed strange or even blasphemous at the time. And rapid AI-driven technological progress would mean that many issues that seem sci-fi are really quite pressing. We want to be open to ideas based on their plausibility and importance, not on whether they are within the current Overton window.
4. **Offer intellectual autonomy:** Though we try to focus on what's most important, there are many different reasonable views on what that is. Senior researchers in particular are encouraged to follow their instinct on what research avenues are most important and fruitful, and to publish freely. There isn't a "party line" on what we believe.

## What you can do

## Engage with our research

We‚Äôd love for you to read our research, discuss the ideas, and criticize them! We‚Äôd also love to see more people working on these topics.

You can follow along by subscribing to our [podcast](https://www.forethought.org/podcast), [RSS feed](https://www.forethought.org/feed), or [Substack](https://forethoughtnewsletter.substack.com/).

Please feel free to [contact us](https://forum.effectivealtruism.org/posts/6JnTAifyqz245Kv7S/) if you are interested in collaborating, or would like our feedback on something (though note that we won‚Äôt be able to substantively engage with all requests).

## Apply to work with us

We are not currently actively hiring (and will likely stay quite small), but we have an [expression of interest form on our site](https://www.forethought.org/careers/expression-of-interest), and would be particularly keen to hear from people who have related research ideas that they would like to pursue.

## Funding

We have funding through to approximately March 2026 at our current size, from two high-net-worth donors.

We‚Äôre looking for $1-2M more, which would help us to diversify funding, make it easier for us to hire more researchers, and extend our runway to 2 years. If you are interested to learn more, please [contact us](https://forum.effectivealtruism.org/posts/6JnTAifyqz245Kv7S/).

---

## Comments

Exciting! Am I right in understanding that Forethought Foundation for Global Priorities Research is no longer operational?

Amrit Sidhu-Brar üî∏

Hi Rockwell!¬†  
  
Yes, in most relevant senses that's correct. We're a new team, we think of ourselves as a new project, and Forethought Foundation's past activities (e.g. its Fellowship programs) and public presence have been wound down. We do have continuity with Forethought Foundation in some ways, mainly legal/administrative.

Reply